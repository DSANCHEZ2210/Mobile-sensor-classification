{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ac6c05f1",
   "metadata": {},
   "outputs": [],
   "source": [
    "#------------------------------------------------------------------------------------------------------------------\n",
    "#   Mobile sensor data acquisition example\n",
    "#------------------------------------------------------------------------------------------------------------------\n",
    "import time\n",
    "import requests\n",
    "import numpy as np\n",
    "import threading\n",
    "\n",
    "import random\n",
    "import pickle\n",
    "from datetime import datetime\n",
    "\n",
    "from scipy.interpolate import interp1d\n",
    "\n",
    "conditions = [('Walking', 0), ('Running', 1), ('Jumping up and down', 2),\n",
    "              ('Greeting', 3), ('Sit down', 4), ('Shadow Boxing', 5)]\n",
    "\n",
    "n_trials = 8      # Menos pruebas por actividad\n",
    "n_windows = 5     # Menos ventanas por prueba\n",
    "window_time = 2   # Igual duración de ventana\n",
    "sampling_rate = 50  # Aumenta frecuencia (más datos por segundo)            # Number of windows for each trial\n",
    "\n",
    "fixation_cross_time = 2     # Time in seconds for attention fixation\n",
    "preparation_time = 2        # Time in seconds for preparation before each trial\n",
    "rest_time = 2               # Time in seconds for rest between trials\n",
    "        # Sampling rate in Hz of the output data\n",
    "max_samp_rate = 5000        # Maximum possible sampling rate\n",
    "max_window_samples = int(window_time*max_samp_rate)     # Maximum number of samples in each window\n",
    "\n",
    "trials = []\n",
    "for _ in range(n_trials):\n",
    "    trials.extend(conditions)\n",
    "random.shuffle(trials)\n",
    "\n",
    "trial_time = fixation_cross_time + preparation_time + n_windows*window_time + rest_time\n",
    "\n",
    "# Communication parameters\n",
    "IP_ADDRESS = '10.43.37.93'  # Replace with your device's IP address\n",
    "COMMAND = 'accX&accY&accZ&acc_time&gyroX&gyroY&gyroZ'\n",
    "BASE_URL = f\"http://{IP_ADDRESS}/get?{COMMAND}\"\n",
    "\n",
    "# Data buffer\n",
    "n_signals = 6   # accX, accY, accZ, gyroX, gyroY, gyroZ\n",
    "buffer_size = int(2 * len(trials)*trial_time * max_samp_rate)       # Buffer size  (2 times the number of samples in the complete experiment)\n",
    "\n",
    "buffer = np.zeros((buffer_size, n_signals + 1), dtype='float64')    # Buffer for storing data (channel 0 is time)\n",
    "buffer_index = 0                                                    # Index for the next data point to be written\n",
    "\n",
    "# Flag for stopping the data acquisition\n",
    "stop_recording_flag = threading.Event()\n",
    "\n",
    "# Mutex for thread-safe access to the buffer\n",
    "buffer_lock = threading.Lock()\n",
    "\n",
    "# Function for continuously fetching data from the mobile device\n",
    "def fetch_data():    \n",
    "    sleep_time = 1. / max_samp_rate \n",
    "    while not stop_recording_flag.is_set():\n",
    "        try:\n",
    "            response = requests.get(BASE_URL, timeout=0.5)\n",
    "            response.raise_for_status()            \n",
    "            data = response.json()\n",
    "\n",
    "            global buffer, buffer_index    \n",
    "            \n",
    "            with buffer_lock:  # Ensure thread-safe access to the buffer\n",
    "                buffer[buffer_index:, 0] = data[\"buffer\"][\"acc_time\"][\"buffer\"][0]\n",
    "                buffer[buffer_index:, 1] = data[\"buffer\"][\"accX\"][\"buffer\"][0]\n",
    "                buffer[buffer_index:, 2] = data[\"buffer\"][\"accY\"][\"buffer\"][0]\n",
    "                buffer[buffer_index:, 3] = data[\"buffer\"][\"accZ\"][\"buffer\"][0]\n",
    "                buffer[buffer_index:, 4] = data[\"buffer\"][\"gyroX\"][\"buffer\"][0]\n",
    "                buffer[buffer_index:, 5] = data[\"buffer\"][\"gyroY\"][\"buffer\"][0]\n",
    "                buffer[buffer_index:, 6] = data[\"buffer\"][\"gyroZ\"][\"buffer\"][0]\n",
    "\n",
    "                buffer_index += 1\n",
    "            \n",
    "        except Exception as e:\n",
    "            print(f\"Error fetching data: {e}\")\n",
    "\n",
    "        time.sleep(sleep_time)\n",
    "\n",
    "# Function for stopping the data acquisition\n",
    "def stop_recording():\n",
    "    stop_recording_flag.set()\n",
    "    recording_thread.join()\n",
    "    \n",
    "# Start data acquisition\n",
    "recording_thread = threading.Thread(target=fetch_data, daemon=True)\n",
    "recording_thread.start()\n",
    "\n",
    "# Run experiment\n",
    "print (\"********* Experiment in progress *********\")    \n",
    "time.sleep(fixation_cross_time)  \n",
    "\n",
    "window_info = []\n",
    "count  = 0\n",
    "for t in trials:\n",
    "\n",
    "    # Fixation cross    \n",
    "    count = count + 1;\n",
    "    print (\"\\n********* Trial {}/{} *********\".format(count, len(trials)))    \n",
    "    time.sleep(fixation_cross_time)    \n",
    "    \n",
    "    # Preparation time\n",
    "    print (t[0])\n",
    "    time.sleep(preparation_time)\n",
    "\n",
    "    # Task\n",
    "    for window in range(n_windows):                \n",
    "        time.sleep(window_time)\n",
    "        #with buffer_lock:  # Ensure thread-safe access to the buffer\n",
    "        window_info.append((t[0], t[1], buffer_index))  \n",
    "\n",
    "    # Rest time    \n",
    "    print (\"----Rest----\")\n",
    "    time.sleep(rest_time)\n",
    "\n",
    "# Stop data acquisition\n",
    "stop_recording()\n",
    "\n",
    "# Calculate average sampling rate\n",
    "t = buffer[:buffer_index, 0]    # Time data\n",
    "diff_t = np.diff(t)             # Time differences\n",
    "\n",
    "print(\"Min sampling rate: {:.2f} Hz\".format(1. / np.max(diff_t)))\n",
    "print(\"Max sampling rate: {:.2f} Hz\".format(1. / np.min(diff_t)))\n",
    "print(\"Average sampling rate: {:.2f} Hz\".format(1. / np.mean(diff_t)))\n",
    "\n",
    "# Interpolation functions for uniform sampling\n",
    "interp_x1 = interp1d(t, buffer[:buffer_index, 1], kind='linear', fill_value=\"extrapolate\")\n",
    "interp_x2 = interp1d(t, buffer[:buffer_index, 2], kind='linear', fill_value=\"extrapolate\")\n",
    "interp_x3 = interp1d(t, buffer[:buffer_index, 3], kind='linear', fill_value=\"extrapolate\")\n",
    "interp_g1 = interp1d(t, buffer[:buffer_index, 4], kind='linear', fill_value=\"extrapolate\")\n",
    "interp_g2 = interp1d(t, buffer[:buffer_index, 5], kind='linear', fill_value=\"extrapolate\")\n",
    "interp_g3 = interp1d(t, buffer[:buffer_index, 6], kind='linear', fill_value=\"extrapolate\")\n",
    "\n",
    "# Separate the data for each trial\n",
    "window_samples = int(sampling_rate * window_time)  # Number of samples in each window\n",
    "data = []\n",
    "for w in window_info:\n",
    "    condition = w[0]        # Condition name\n",
    "    condition_id = w[1]     # Condition ID\n",
    "    start_index =  w[2]     # Start index of the window in the buffer\n",
    "\n",
    "    # Calculate the uniform time vector for the window\n",
    "    t_start = buffer[start_index, 0]    # Start time of the window\n",
    "    t_uniform = np.linspace(t_start, t_start + window_time, int(window_time * sampling_rate))    \n",
    "\n",
    "    # Interpolate the signals for the uniform time vector\n",
    "    signal_data = np.column_stack((\n",
    "    interp_x1(t_uniform), interp_x2(t_uniform), interp_x3(t_uniform),\n",
    "    interp_g1(t_uniform), interp_g2(t_uniform), interp_g3(t_uniform)\n",
    "))\n",
    "    \n",
    "    # Append the data for this window\n",
    "    data.append((condition, condition_id, signal_data))\n",
    "\n",
    "\n",
    "nombre = \"vertiz_d\"  \n",
    "outputFile = open(f\"{nombre}_data.obj\", 'wb')\n",
    "pickle.dump(data, outputFile)\n",
    "outputFile.close()\n",
    "\n",
    "#------------------------------------------------------------------------------------------------------------------\n",
    "#   End of file\n",
    "#------------------------------------------------------------------------------------------------------------------"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "60d53fa5",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "\n",
    "# File names to combine\n",
    "archivos = ['daniel_sancho_data.obj','gustavo_data.obj', 'vertiz_d_data.obj']\n",
    "\n",
    "# List to hold all data\n",
    "all_data = []\n",
    "\n",
    "for archivo in archivos:\n",
    "    with open(archivo, 'rb') as f:\n",
    "        data = pickle.load(f)\n",
    "        all_data.extend(data)\n",
    "\n",
    "with open('data_combined.obj', 'wb') as f:\n",
    "    pickle.dump(all_data, f)\n",
    "\n",
    "print(\"Combined data saved to 'datos_combined_new.obj'.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b51b1892",
   "metadata": {},
   "outputs": [],
   "source": [
    "#------------------------------------------------------------------------------------------------------------------\n",
    "#   Mobile sensor data acquisition and processing\n",
    "#------------------------------------------------------------------------------------------------------------------\n",
    "import pickle\n",
    "import numpy as np\n",
    "from scipy import stats\n",
    "\n",
    "# Load the combined data file\n",
    "file_name = 'data_combined.obj'\n",
    "with open(file_name, 'rb') as inputFile:\n",
    "    experiment_data = pickle.load(inputFile)\n",
    "\n",
    "# Helper function: calculate signal energy\n",
    "def energy(sig):\n",
    "    return np.sum(sig ** 2) / len(sig)\n",
    "\n",
    "# Helper function: count zero crossings in the signal\n",
    "def zero_crossings(sig):\n",
    "    return ((sig[:-1] * sig[1:]) < 0).sum()\n",
    "\n",
    "# List to store feature vectors\n",
    "features = []\n",
    "\n",
    "# Process each trial\n",
    "for tr in experiment_data:\n",
    "    label = tr[1]       # Activity ID (e.g., 0, 1, ..., 5)\n",
    "    signal = tr[2]      # Signal shape: (samples, 6 axes: accX, accY, accZ, gyroX, gyroY, gyroZ)\n",
    "\n",
    "    feat = [label]      # Start the feature list with the activity label\n",
    "\n",
    "    for i in range(6):  # Loop through each axis: accX, accY, accZ, gyroX, gyroY, gyroZ\n",
    "        sig = signal[:, i]\n",
    "\n",
    "        feat.append(np.mean(sig))                # Mean\n",
    "        feat.append(np.std(sig))                 # Standard deviation\n",
    "        feat.append(np.max(sig))                 # Maximum value\n",
    "        feat.append(np.min(sig))                 # Minimum value\n",
    "        feat.append(stats.skew(sig))             # Skewness\n",
    "        feat.append(stats.kurtosis(sig))         # Kurtosis\n",
    "        feat.append(energy(sig))                 # Energy\n",
    "        feat.append(np.sqrt(np.mean(sig ** 2)))  # RMS\n",
    "        feat.append(zero_crossings(sig))         # Zero-crossing count\n",
    "\n",
    "    acc_mag = np.linalg.norm(signal[:, 0:3], axis=1)  # Magnitude of accX, accY, accZ\n",
    "    gyro_mag = np.linalg.norm(signal[:, 3:6], axis=1)  # Magnitude of gyroX, gyroY, gyroZ\n",
    "\n",
    "    feat.append(np.mean(acc_mag))        # Mean acc magnitude\n",
    "    feat.append(np.std(acc_mag))         # Std acc magnitude\n",
    "    feat.append(energy(acc_mag))         # Energy acc magnitude\n",
    "\n",
    "    feat.append(np.mean(gyro_mag))       # Mean gyro magnitude\n",
    "    feat.append(np.std(gyro_mag))        # Std gyro magnitude\n",
    "    feat.append(energy(gyro_mag))         # Energy of magnitude\n",
    "\n",
    "    features.append(feat)\n",
    "\n",
    "# Convert list to NumPy array\n",
    "processed_data = np.array(features)\n",
    "\n",
    "# Separate features and labels\n",
    "x = processed_data[:, 1:]  # Features: 6 axes × 9 stats + 6 global (acc/gyro mag) = 60 total\n",
    "y = processed_data[:, 0]   # Labels (activity ID)\n",
    "\n",
    "# Save processed data to file\n",
    "np.savetxt(\"activity_new_data.txt\", processed_data, delimiter=\",\", fmt=\"%.5f\")\n",
    "\n",
    "print(\"Data saved to 'activity_new_data.txt'\")\n",
    "\n",
    "#------------------------------------------------------------------------------------------------------------------\n",
    "#   End of file\n",
    "#--------------------------------------------"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "31bd3db3",
   "metadata": {},
   "source": [
    "After all model evaluation... (models.ipynb and New_Models.ipynb)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "374986a0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================\n",
    "# Optimized and Calibrated Stacking Ensemble\n",
    "# ============================================\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\", message=\".*use_label_encoder.*\")\n",
    "import numpy as np\n",
    "from sklearn.model_selection import RepeatedStratifiedKFold, cross_val_score, RandomizedSearchCV\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.feature_selection import SelectKBest, f_classif\n",
    "from sklearn.calibration import CalibratedClassifierCV\n",
    "from sklearn.ensemble import GradientBoostingClassifier, ExtraTreesClassifier, StackingClassifier\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from xgboost import XGBClassifier\n",
    "from scipy.stats import uniform, randint\n",
    "from collections import Counter\n",
    "\n",
    "# -----------------------\n",
    "# 1. Load dataset\n",
    "# -----------------------\n",
    "data = np.loadtxt('activity_data_clean.txt')\n",
    "X = data[:, 1:]\n",
    "y = data[:, 0].astype(int)\n",
    "print(Counter(y))\n",
    "\n",
    "# -----------------------\n",
    "# 2. Feature Selection\n",
    "# -----------------------\n",
    "feature_selector = SelectKBest(score_func=f_classif, k='all')  # keep all features\n",
    "\n",
    "# -----------------------\n",
    "# 3. Base Classifiers\n",
    "# -----------------------\n",
    "xgb = XGBClassifier(eval_metric='mlogloss', random_state=42)\n",
    "gb = GradientBoostingClassifier(random_state=42)\n",
    "et = ExtraTreesClassifier(class_weight='balanced', random_state=42)\n",
    "\n",
    "# -----------------------\n",
    "# 4. Tuning Space (XGBoost)\n",
    "# -----------------------\n",
    "param_dist_xgb = {\n",
    "    'clf__n_estimators': randint(200, 400),\n",
    "    'clf__learning_rate': uniform(0.01, 0.2),\n",
    "    'clf__max_depth': randint(3, 8),\n",
    "    'clf__subsample': uniform(0.6, 0.4),\n",
    "    'clf__colsample_bytree': uniform(0.6, 0.4)\n",
    "}\n",
    "\n",
    "# -----------------------\n",
    "# 5. Pipeline for Random Search\n",
    "# -----------------------\n",
    "pipeline = Pipeline([\n",
    "    ('scaler', StandardScaler()),\n",
    "    ('select', feature_selector),\n",
    "    ('clf', xgb)\n",
    "])\n",
    "\n",
    "# -----------------------\n",
    "# 6. Randomized Search\n",
    "# -----------------------\n",
    "rskf = RepeatedStratifiedKFold(n_splits=5, n_repeats=3, random_state=42)\n",
    "random_search = RandomizedSearchCV(\n",
    "    pipeline,\n",
    "    param_distributions=param_dist_xgb,\n",
    "    n_iter=60,\n",
    "    cv=rskf,\n",
    "    scoring='accuracy',\n",
    "    n_jobs=-1,\n",
    "    verbose=1\n",
    ")\n",
    "random_search.fit(X, y)\n",
    "best_xgb = random_search.best_estimator_\n",
    "\n",
    "# -----------------------\n",
    "# 7. Calibrate XGBoost\n",
    "# -----------------------\n",
    "calibrated_xgb = CalibratedClassifierCV(estimator=best_xgb, method='sigmoid', cv=5)\n",
    "calibrated_xgb.fit(X, y)\n",
    "\n",
    "# -----------------------\n",
    "# 8. Stacking Ensemble\n",
    "# -----------------------\n",
    "stacking_model = StackingClassifier(\n",
    "    estimators=[\n",
    "        ('xgb', calibrated_xgb),\n",
    "        ('gb', gb),\n",
    "        ('et', et)\n",
    "    ],\n",
    "    final_estimator=LogisticRegression(),\n",
    "    cv=5,\n",
    "    n_jobs=-1\n",
    ")\n",
    "\n",
    "# -----------------------\n",
    "# 9. Evaluate\n",
    "# -----------------------\n",
    "stacking_scores = cross_val_score(stacking_model, X, y, cv=5, scoring='accuracy')\n",
    "print(f\"\\nStacking Ensemble Accuracy: {stacking_scores.mean():.4f} ± {stacking_scores.std():.4f}\")\n",
    "print(\"\\nBest XGBoost parameters found:\")\n",
    "print(random_search.best_params_)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0249ccbd",
   "metadata": {},
   "source": [
    "# Final Online Classification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2bf93380",
   "metadata": {},
   "outputs": [],
   "source": [
    "#------------------------------------------------------------------------------------------------------------------\n",
    "#   Online classification of mobile sensor data (accelerometer only, 30 best features embedded in training)\n",
    "#------------------------------------------------------------------------------------------------------------------\n",
    "\n",
    "import time\n",
    "import requests\n",
    "import numpy as np\n",
    "import threading\n",
    "from scipy.interpolate import interp1d\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.feature_selection import SelectKBest, f_classif\n",
    "from xgboost import XGBClassifier\n",
    "from scipy import stats\n",
    "\n",
    "##########################################\n",
    "############ Data properties #############\n",
    "##########################################\n",
    "\n",
    "sampling_rate = 20\n",
    "window_time = 0.5\n",
    "window_samples = int(window_time * sampling_rate)\n",
    "\n",
    "##########################################\n",
    "##### Load data and train model here #####\n",
    "##########################################\n",
    "\n",
    "data = np.loadtxt(\"activity_new_data.txt\", delimiter=\",\")\n",
    "X = data[:, 1:]\n",
    "y = data[:, 0].astype(int)\n",
    "\n",
    "pipeline = Pipeline([\n",
    "    ('scaler', StandardScaler()),\n",
    "    ('select', SelectKBest(score_func=f_classif, k=30)),\n",
    "    ('clf', XGBClassifier(\n",
    "        colsample_bytree=0.8,\n",
    "        learning_rate=0.05,\n",
    "        max_depth=4,\n",
    "        n_estimators=200,\n",
    "        subsample=0.8,\n",
    "        eval_metric='mlogloss',\n",
    "    ))\n",
    "])\n",
    "\n",
    "pipeline.fit(X, y)  # ENTRENAMIENTO REAL con solo 30 columnas\n",
    "\n",
    "\n",
    "##########################################\n",
    "##### Data acquisition configuration #####\n",
    "##########################################\n",
    "\n",
    "IP_ADDRESS = '10.43.37.93'\n",
    "COMMAND = 'accX&accY&accZ&gyroX&gyroY&gyroZ&acc_time'\n",
    "BASE_URL = f\"http://{IP_ADDRESS}/get?{COMMAND}\"\n",
    "\n",
    "max_samp_rate = 5000\n",
    "n_signals = 6  \n",
    "buffer_size = max_samp_rate * 5\n",
    "\n",
    "buffer = np.zeros((buffer_size, n_signals + 1), dtype='float64')\n",
    "buffer_index = 0\n",
    "last_sample_time = 0.0\n",
    "\n",
    "stop_recording_flag = threading.Event()\n",
    "buffer_lock = threading.Lock()\n",
    "\n",
    "def fetch_data():\n",
    "    sleep_time = 1. / max_samp_rate\n",
    "    while not stop_recording_flag.is_set():\n",
    "        try:\n",
    "            response = requests.get(BASE_URL, timeout=0.5)\n",
    "            response.raise_for_status()\n",
    "            data = response.json()\n",
    "\n",
    "            global buffer, buffer_index, last_sample_time\n",
    "\n",
    "            with buffer_lock:\n",
    "                buffer[buffer_index, 0] = data[\"buffer\"][\"acc_time\"][\"buffer\"][0]\n",
    "                buffer[buffer_index, 1] = data[\"buffer\"][\"accX\"][\"buffer\"][0]\n",
    "                buffer[buffer_index, 2] = data[\"buffer\"][\"accY\"][\"buffer\"][0]\n",
    "                buffer[buffer_index, 3] = data[\"buffer\"][\"accZ\"][\"buffer\"][0]\n",
    "                buffer[buffer_index:, 4] = data[\"buffer\"][\"gyroX\"][\"buffer\"][0]\n",
    "                buffer[buffer_index:, 5] = data[\"buffer\"][\"gyroY\"][\"buffer\"][0]\n",
    "                buffer[buffer_index:, 6] = data[\"buffer\"][\"gyroZ\"][\"buffer\"][0]\n",
    "\n",
    "                buffer_index = (buffer_index + 1) % buffer_size\n",
    "                last_sample_time = data[\"buffer\"][\"acc_time\"][\"buffer\"][0]\n",
    "\n",
    "        except Exception as e:\n",
    "            print(f\"Error fetching data: {e}\")\n",
    "        time.sleep(sleep_time)\n",
    "\n",
    "def stop_recording():\n",
    "    stop_recording_flag.set()\n",
    "    recording_thread.join()\n",
    "\n",
    "recording_thread = threading.Thread(target=fetch_data, daemon=True)\n",
    "recording_thread.start()\n",
    "\n",
    "##########################################\n",
    "######### Online classification ##########\n",
    "##########################################\n",
    "\n",
    "def energy(sig):\n",
    "    return np.sum(sig ** 2) / len(sig)\n",
    "\n",
    "def zero_crossings(sig):\n",
    "    return ((sig[:-1] * sig[1:]) < 0).sum()\n",
    "\n",
    "activity_map = {\n",
    "    0: \"Walking\",\n",
    "    1: \"Running\",\n",
    "    2: \"Jumping Up and Down\",\n",
    "    3: \"Greeting\",\n",
    "    4: \"Sit Down\",\n",
    "    5: \"Shadow Boxing\"\n",
    "}\n",
    "\n",
    "update_time = 0.25\n",
    "ref_time = time.time()\n",
    "\n",
    "while True:\n",
    "    time.sleep(update_time)\n",
    "\n",
    "    if buffer_index > 2 * sampling_rate:\n",
    "        ref_time = time.time()\n",
    "\n",
    "        end_index = (buffer_index - 1) % buffer_size\n",
    "        start_index = (buffer_index - 2) % buffer_size\n",
    "\n",
    "        with buffer_lock:\n",
    "            while (buffer[end_index, 0] - buffer[start_index, 0]) <= window_time:\n",
    "                start_index = (start_index - 1) % buffer_size\n",
    "\n",
    "            indices = (buffer_index - np.arange(buffer_size, 0, -1)) % buffer_size\n",
    "            last_raw_data = buffer[indices, :]\n",
    "\n",
    "        t = last_raw_data[:, 0]\n",
    "        t_uniform = np.linspace(last_sample_time - window_time, last_sample_time, int(window_time * sampling_rate))\n",
    "\n",
    "        last_data = np.zeros((len(t_uniform), n_signals))\n",
    "        for i in range(n_signals):\n",
    "            interp_x = interp1d(t, last_raw_data[:, i+1], kind='linear', fill_value=\"extrapolate\")\n",
    "            last_data[:, i] = interp_x(t_uniform)\n",
    "\n",
    "        features = []\n",
    "        for i in range(6):\n",
    "            sig = last_data[:, i]\n",
    "            features.extend([\n",
    "                np.mean(sig),\n",
    "                np.std(sig),\n",
    "                np.max(sig),\n",
    "                np.min(sig),\n",
    "                stats.skew(sig),\n",
    "                stats.kurtosis(sig),\n",
    "                energy(sig),\n",
    "                np.sqrt(np.mean(sig ** 2)),\n",
    "                zero_crossings(sig)\n",
    "            ])\n",
    "\n",
    "        acc_mag = np.linalg.norm(last_data[:, 0:3], axis=1)\n",
    "        gyro_mag = np.linalg.norm(last_data[:, 3:6], axis=1)\n",
    "        features.extend([\n",
    "        np.mean(acc_mag),\n",
    "        np.std(acc_mag),\n",
    "        energy(acc_mag),\n",
    "        np.mean(gyro_mag),\n",
    "        np.std(gyro_mag),\n",
    "        energy(gyro_mag)\n",
    "        ])\n",
    "\n",
    "\n",
    "\n",
    "        X_new = np.array(features).reshape(1, -1)\n",
    "        prediction = pipeline.predict(X_new)\n",
    "        activity_label = activity_map.get(prediction[0], \"Unknown\")\n",
    "        print(f\"Predicción en tiempo real: {activity_label}\")\n",
    "\n",
    "stop_recording()\n",
    "\n",
    "#------------------------------------------------------------------------------------------------------------------\n",
    "#   End of file\n",
    "#-------------------------------------------------"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
